{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYtJxkhKpYK2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "Решим задачу семантической классификации твитов.  \n",
    "Для этого мы воспользуемся предобученными эмбеддингами word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBOdoFS8AdpP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Для начала скачаем датасет для семантической классификации твитов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wXjhtsfF_gBK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gdown.download('https://drive.google.com/uc?id=1eE1FiUkXkcbw0McId4i7qY-L8hH-_Qph&export=download')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!unzip archive.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sh6wW-K53Mle",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Импортируем нужные библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "A2Y5CHRm6NFe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import torch\n",
    "import nltk\n",
    "import gensim\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim.downloader as api\n",
    "from torch.utils.data import Dataset, random_split\n",
    "\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Зафиксируем `random`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.random.manual_seed(42)\n",
    "torch.cuda.random.manual_seed(42)\n",
    "torch.cuda.random.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Определим `device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "73Lb0wbESrgQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "L_Wv-4bu83Fl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", \n",
    "                   encoding=\"latin\", \n",
    "                   header=None, \n",
    "                   names=[\"emotion\", \"id\", \"date\", \"flag\", \"user\", \"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RY1pvYDS3Yuj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Посмотрим на данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jST2tjgjCTWD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emotion          id                          date      flag  \\\n",
       "0        0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Выведем некоторые статистики:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.600000e+06</td>\n",
       "      <td>1.600000e+06</td>\n",
       "      <td>1600000</td>\n",
       "      <td>1600000</td>\n",
       "      <td>1600000</td>\n",
       "      <td>1600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>774363</td>\n",
       "      <td>1</td>\n",
       "      <td>659775</td>\n",
       "      <td>1581466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mon Jun 15 12:53:14 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>lost_dog</td>\n",
       "      <td>isPlayer Has Died! Sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "      <td>1600000</td>\n",
       "      <td>549</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.998818e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.000001e+00</td>\n",
       "      <td>1.935761e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.467810e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.956916e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>2.002102e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.177059e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.329206e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             emotion            id                          date      flag  \\\n",
       "count   1.600000e+06  1.600000e+06                       1600000   1600000   \n",
       "unique           NaN           NaN                        774363         1   \n",
       "top              NaN           NaN  Mon Jun 15 12:53:14 PDT 2009  NO_QUERY   \n",
       "freq             NaN           NaN                            20   1600000   \n",
       "mean    2.000000e+00  1.998818e+09                           NaN       NaN   \n",
       "std     2.000001e+00  1.935761e+08                           NaN       NaN   \n",
       "min     0.000000e+00  1.467810e+09                           NaN       NaN   \n",
       "25%     0.000000e+00  1.956916e+09                           NaN       NaN   \n",
       "50%     2.000000e+00  2.002102e+09                           NaN       NaN   \n",
       "75%     4.000000e+00  2.177059e+09                           NaN       NaN   \n",
       "max     4.000000e+00  2.329206e+09                           NaN       NaN   \n",
       "\n",
       "            user                       text  \n",
       "count    1600000                    1600000  \n",
       "unique    659775                    1581466  \n",
       "top     lost_dog  isPlayer Has Died! Sorry   \n",
       "freq         549                        210  \n",
       "mean         NaN                        NaN  \n",
       "std          NaN                        NaN  \n",
       "min          NaN                        NaN  \n",
       "25%          NaN                        NaN  \n",
       "50%          NaN                        NaN  \n",
       "75%          NaN                        NaN  \n",
       "max          NaN                        NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhbR5JJyA2VW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Выведем несколько примеров твитов, чтобы понимать, с чем мы имеем дело:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kCBwe0wR83C2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@chrishasboobs AHHH I HOPE YOUR OK!!! \n",
      "@misstoriblack cool , i have no tweet apps  for my razr 2\n",
      "@TiannaChaos i know  just family drama. its lame.hey next time u hang out with kim n u guys like have a sleepover or whatever, ill call u\n",
      "School email won't open  and I have geography stuff on there to revise! *Stupid School* :'(\n",
      "upper airways problem \n",
      "Going to miss Pastor's sermon on Faith... \n",
      "on lunch....dj should come eat with me \n",
      "@piginthepoke oh why are you feeling like that? \n",
      "gahh noo!peyton needs to live!this is horrible \n",
      "@mrstessyman thank you glad you like it! There is a product review bit on the site  Enjoy knitting it!\n"
     ]
    }
   ],
   "source": [
    "examples = data[\"text\"].sample(10)\n",
    "print(\"\\n\".join(examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvcYW8aX3mKt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Как видим, тексты твитов очень \"грязные\". Нужно предобработать датасет, прежде чем строить для него модель классификации.\n",
    "\n",
    "Чтобы сравнивать различные методы обработки текста/модели/прочее, разделим датасет на dev(для обучения модели) и test(для получения качества модели)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "f8hUK-jnQg6O",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "indexes = np.arange(data.shape[0])\n",
    "np.random.shuffle(indexes)\n",
    "dev_size = math.ceil(data.shape[0] * 0.8)\n",
    "\n",
    "dev_indexes = indexes[:dev_size]\n",
    "test_indexes = indexes[dev_size:]\n",
    "\n",
    "dev_data = data.iloc[dev_indexes]\n",
    "test_data = data.iloc[test_indexes]\n",
    "\n",
    "dev_data.reset_index(drop=True, inplace=True)\n",
    "test_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ivcpeFoCnZA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Обработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Df4nca285Dar",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Токенизируем текст, избавимся от знаков пунктуации и выкинем все слова, состоящие менее чем из 4 букв:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nsNHNDES9ZVF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ claire_nelson i ' m on the north devon coast the next few weeks will be down in devon again in may sometime i hope though !\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.WordPunctTokenizer()\n",
    "line = tokenizer.tokenize(dev_data[\"text\"][0].lower())\n",
    "print(\" \".join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GcBS_u_hTuxp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "north devon coast next weeks will down devon again sometime hope though\n"
     ]
    }
   ],
   "source": [
    "filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) > 3]\n",
    "print(\" \".join(filtered_line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuFmlXkC6E7X",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Загрузим предобученную модель эмбеддингов. \n",
    "Данная модель выдает эмбеддинги для **слов**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "cACJpje2T5bc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word2vec = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "NafmYHrkT5YD",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "emb_line = [word2vec.get_vector(w) for w in filtered_line if w in word2vec]\n",
    "print(sum(emb_line).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTS6LCkd6_E7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Нормализуем эмбеддинги:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3PyLTZ6xf3Oq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "[False, False, False, False, False, False, False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "mean = np.mean(word2vec.vectors, axis=0)\n",
    "std = np.std(word2vec.vectors, axis=0)\n",
    "norm_emb_line = [(word2vec.get_vector(w) - mean) / std for w in filtered_line if w in word2vec and len(w) > 3]\n",
    "print(sum(norm_emb_line).shape)\n",
    "print([all(norm_emb_line[i] == emb_line[i]) for i in range(len(emb_line))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7vm6Ppd7Ubw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Сделаем датасет, который будет по запросу возвращать подготовленные данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "b4eZajF7pZ1X",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TwitterDataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec):\n",
    "        self.tokenizer = nltk.WordPunctTokenizer()\n",
    "        self.data = data\n",
    "        self.feature_column = feature_column\n",
    "        self.target_column = target_column\n",
    "        self.word2vec = word2vec\n",
    "        self.label2num = lambda label: 0 if label == 0 else 1\n",
    "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
    "        self.std = np.std(word2vec.vectors, axis=0)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.data[self.feature_column][item]\n",
    "        label = self.label2num(self.data[self.target_column][item])\n",
    "        tokens = self.get_tokens_(text)\n",
    "        embeddings = self.get_embeddings_(tokens)\n",
    "\n",
    "        return {\"feature\": embeddings, \"target\": label}\n",
    "\n",
    "    def get_tokens_(self, text):\n",
    "        # Получим все токены из текста и профильтруем их\n",
    "        text = re.sub(r'@\\S+', '', text)           # уберём ники вида @chrishasboobs\n",
    "        text = re.sub(r'http\\S+', '', text)        # уберём http ссылки\n",
    "        text = re.sub('[^A-Za-z0-9]+', ' ', text)  # уберём слова в неверной кодировке\n",
    "        # заменим все символы, которые повторяются >3 раз подряд на единичные (stoppp -> stop)\n",
    "        text = re.sub(r'([A-Za-z0-9])\\1(?=\\1)', '', text)  \n",
    "        line = self.tokenizer.tokenize(text.lower())\n",
    "        filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) > 2]\n",
    "        \n",
    "        return filtered_line\n",
    "\n",
    "    def get_embeddings_(self, tokens):\n",
    "        # Получим эмбеддинги слов и нормализуем их\n",
    "        embeddings = [(self.word2vec.get_vector(w) - self.mean) / self.std for w in tokens if w in self.word2vec]\n",
    "\n",
    "        if len(embeddings) == 0:\n",
    "            embeddings = np.zeros((1, self.word2vec.vector_size))\n",
    "        else:\n",
    "            embeddings = np.array(embeddings)\n",
    "            if len(embeddings.shape) == 1:\n",
    "                embeddings = embeddings.reshape(-1, 1)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "IZJpttbXpZyz",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dev = TwitterDataset(dev_data, \"text\", \"emotion\", word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4AhHrWa196Yc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Average embedding (2 балла)\n",
    "---\n",
    "Вектор предложения есть средний вектор всех слов в предложeнии (которые остались после токенизации и удаления коротких слов, конечно). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ScdokSW-994t",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1280\n"
     ]
    }
   ],
   "source": [
    "indexes = np.arange(len(dev))\n",
    "np.random.shuffle(indexes)\n",
    "example_indexes = indexes[::1000]\n",
    "\n",
    "examples = {\"features\": [np.mean(dev[i][\"feature\"], axis=0) for i in example_indexes], \n",
    "            \"targets\": [dev[i][\"target\"] for i in example_indexes]}\n",
    "print(len(examples[\"features\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yGQ_lOx_1NL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Давайте сделаем визуализацию полученных векторов твитов тренировочного (dev) датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne = TSNE(n_components=2, n_jobs=-1)\n",
    "examples[\"transformed_features\"] = tsne.fit_transform(examples[\"features\"])  # Обучим TSNE на эмбеддингах слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "aKFZRSHdtIac",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# examples[\"transformed_features\"] = pca.fit_transform(examples[\"features\"])  # Обучим PCA на эмбеддингах слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "szEOWdiNtIX8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.1.min.js\"];\n",
       "  const css_urls = [];\n",
       "  \n",
       "\n",
       "  const inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.1.min.js\"];\n  const css_urls = [];\n  \n\n  const inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import bokeh.models as bm, bokeh.plotting as pl\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
    "                 width=600, height=400, show=True, **kwargs):\n",
    "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
    "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
    "\n",
    "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
    "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
    "\n",
    "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
    "    if show: pl.show(fig)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "7OONK8ldtIWe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"347b07c8-85bc-436c-a36b-cc20ee33d7ce\" data-root-id=\"1004\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  const docs_json = {\"0d9486a0-e771-47cf-a580-9129740c3420\":{\"defs\":[],\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1013\"}],\"center\":[{\"id\":\"1016\"},{\"id\":\"1020\"}],\"height\":400,\"left\":[{\"id\":\"1017\"}],\"renderers\":[{\"id\":\"1039\"}],\"title\":{\"id\":\"1043\"},\"toolbar\":{\"id\":\"1028\"},\"x_range\":{\"id\":\"1005\"},\"x_scale\":{\"id\":\"1009\"},\"y_range\":{\"id\":\"1007\"},\"y_scale\":{\"id\":\"1011\"}},\"id\":\"1004\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"axis\":{\"id\":\"1013\"},\"coordinates\":null,\"group\":null,\"ticker\":null},\"id\":\"1016\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1051\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"coordinates\":null,\"formatter\":{\"id\":\"1049\"},\"group\":null,\"major_label_policy\":{\"id\":\"1050\"},\"ticker\":{\"id\":\"1014\"}},\"id\":\"1013\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1052\",\"type\":\"Selection\"},{\"attributes\":{\"source\":{\"id\":\"1003\"}},\"id\":\"1040\",\"type\":\"CDSView\"},{\"attributes\":{\"data\":{\"color\":[\"red\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"red\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"blue\",\"red\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"red\",\"red\",\"blue\",\"blue\",\"blue\",\"red\"],\"x\":{\"__ndarray__\":\"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\",\"dtype\":\"float32\",\"order\":\"little\",\"shape\":[1280]},\"y\":{\"__ndarray__\":\"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\",\"dtype\":\"float32\",\"order\":\"little\",\"shape\":[1280]}},\"selected\":{\"id\":\"1052\"},\"selection_policy\":{\"id\":\"1051\"}},\"id\":\"1003\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"1050\",\"type\":\"AllLabels\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.25},\"fill_color\":{\"field\":\"color\"},\"hatch_alpha\":{\"value\":0.25},\"hatch_color\":{\"field\":\"color\"},\"line_alpha\":{\"value\":0.25},\"line_color\":{\"field\":\"color\"},\"size\":{\"value\":10},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1036\",\"type\":\"Scatter\"},{\"attributes\":{\"active_scroll\":{\"id\":\"1022\"},\"tools\":[{\"id\":\"1021\"},{\"id\":\"1022\"},{\"id\":\"1023\"},{\"id\":\"1024\"},{\"id\":\"1025\"},{\"id\":\"1026\"},{\"id\":\"1041\"}]},\"id\":\"1028\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"1024\",\"type\":\"SaveTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"field\":\"color\"},\"hatch_alpha\":{\"value\":0.2},\"hatch_color\":{\"field\":\"color\"},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"field\":\"color\"},\"size\":{\"value\":10},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1038\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1049\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"1018\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1025\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1005\",\"type\":\"DataRange1d\"},{\"attributes\":{\"coordinates\":null,\"group\":null},\"id\":\"1043\",\"type\":\"Title\"},{\"attributes\":{\"bottom_units\":\"screen\",\"coordinates\":null,\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"group\":null,\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"syncable\":false,\"top_units\":\"screen\"},\"id\":\"1027\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"field\":\"color\"},\"hatch_alpha\":{\"value\":0.1},\"hatch_color\":{\"field\":\"color\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"field\":\"color\"},\"size\":{\"value\":10},\"x\":{\"field\":\"x\"},\"y\":{\"field\":\"y\"}},\"id\":\"1037\",\"type\":\"Scatter\"},{\"attributes\":{},\"id\":\"1047\",\"type\":\"AllLabels\"},{\"attributes\":{},\"id\":\"1021\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"LinearScale\"},{\"attributes\":{\"coordinates\":null,\"formatter\":{\"id\":\"1046\"},\"group\":null,\"major_label_policy\":{\"id\":\"1047\"},\"ticker\":{\"id\":\"1018\"}},\"id\":\"1017\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"1046\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"1003\"},\"glyph\":{\"id\":\"1036\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"1038\"},\"nonselection_glyph\":{\"id\":\"1037\"},\"view\":{\"id\":\"1040\"}},\"id\":\"1039\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"1022\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1014\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"HelpTool\"},{\"attributes\":{\"overlay\":{\"id\":\"1027\"}},\"id\":\"1023\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1007\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"LinearScale\"},{\"attributes\":{\"axis\":{\"id\":\"1017\"},\"coordinates\":null,\"dimension\":1,\"group\":null,\"ticker\":null},\"id\":\"1020\",\"type\":\"Grid\"},{\"attributes\":{\"callback\":null,\"tooltips\":[]},\"id\":\"1041\",\"type\":\"HoverTool\"}],\"root_ids\":[\"1004\"]},\"title\":\"Bokeh Application\",\"version\":\"2.4.1\"}};\n",
       "  const render_items = [{\"docid\":\"0d9486a0-e771-47cf-a580-9129740c3420\",\"root_ids\":[\"1004\"],\"roots\":{\"1004\":\"347b07c8-85bc-436c-a36b-cc20ee33d7ce\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    let attempts = 0;\n",
       "    const timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1004"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div style=\"display: table;\"><div style=\"display: table-row;\"><div style=\"display: table-cell;\"><b title=\"bokeh.plotting.figure.Figure\">Figure</b>(</div><div style=\"display: table-cell;\">id&nbsp;=&nbsp;'1004', <span id=\"1110\" style=\"cursor: pointer;\">&hellip;)</span></div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">above&nbsp;=&nbsp;[],</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">align&nbsp;=&nbsp;'start',</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">aspect_ratio&nbsp;=&nbsp;None,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">aspect_scale&nbsp;=&nbsp;1,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">background&nbsp;=&nbsp;None,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">background_fill_alpha&nbsp;=&nbsp;1.0,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">background_fill_color&nbsp;=&nbsp;'#ffffff',</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">below&nbsp;=&nbsp;[LinearAxis(id='1013', ...)],</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">border_fill_alpha&nbsp;=&nbsp;1.0,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">border_fill_color&nbsp;=&nbsp;'#ffffff',</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">center&nbsp;=&nbsp;[Grid(id='1016', ...), Grid(id='1020', ...)],</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">css_classes&nbsp;=&nbsp;[],</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">disabled&nbsp;=&nbsp;False,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">extra_x_ranges&nbsp;=&nbsp;{},</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">extra_x_scales&nbsp;=&nbsp;{},</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">extra_y_ranges&nbsp;=&nbsp;{},</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">extra_y_scales&nbsp;=&nbsp;{},</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">frame_height&nbsp;=&nbsp;None,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">frame_width&nbsp;=&nbsp;None,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">height&nbsp;=&nbsp;400,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">height_policy&nbsp;=&nbsp;'auto',</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">hidpi&nbsp;=&nbsp;True,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">inner_height&nbsp;=&nbsp;0,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">inner_width&nbsp;=&nbsp;0,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">js_event_callbacks&nbsp;=&nbsp;{},</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">js_property_callbacks&nbsp;=&nbsp;{},</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">left&nbsp;=&nbsp;[LinearAxis(id='1017', ...)],</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">lod_factor&nbsp;=&nbsp;10,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">lod_interval&nbsp;=&nbsp;300,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">lod_threshold&nbsp;=&nbsp;2000,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">lod_timeout&nbsp;=&nbsp;500,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">margin&nbsp;=&nbsp;(0, 0, 0, 0),</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">match_aspect&nbsp;=&nbsp;False,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">max_height&nbsp;=&nbsp;None,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">max_width&nbsp;=&nbsp;None,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border&nbsp;=&nbsp;5,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border_bottom&nbsp;=&nbsp;None,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border_left&nbsp;=&nbsp;None,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border_right&nbsp;=&nbsp;None,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_border_top&nbsp;=&nbsp;None,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_height&nbsp;=&nbsp;None,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_width&nbsp;=&nbsp;None,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">name&nbsp;=&nbsp;None,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outer_height&nbsp;=&nbsp;0,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outer_width&nbsp;=&nbsp;0,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_alpha&nbsp;=&nbsp;1.0,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_cap&nbsp;=&nbsp;'butt',</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_color&nbsp;=&nbsp;'#e5e5e5',</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_dash&nbsp;=&nbsp;[],</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_dash_offset&nbsp;=&nbsp;0,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_join&nbsp;=&nbsp;'bevel',</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">outline_line_width&nbsp;=&nbsp;1,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">output_backend&nbsp;=&nbsp;'canvas',</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">renderers&nbsp;=&nbsp;[GlyphRenderer(id='1039', ...)],</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">reset_policy&nbsp;=&nbsp;'standard',</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">right&nbsp;=&nbsp;[],</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">sizing_mode&nbsp;=&nbsp;None,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">subscribed_events&nbsp;=&nbsp;[],</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">syncable&nbsp;=&nbsp;True,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">tags&nbsp;=&nbsp;[],</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">title&nbsp;=&nbsp;Title(id='1043', ...),</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">title_location&nbsp;=&nbsp;'above',</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">toolbar&nbsp;=&nbsp;Toolbar(id='1028', ...),</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">toolbar_location&nbsp;=&nbsp;'right',</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">toolbar_sticky&nbsp;=&nbsp;True,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">visible&nbsp;=&nbsp;True,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">width&nbsp;=&nbsp;600,</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">width_policy&nbsp;=&nbsp;'auto',</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">x_range&nbsp;=&nbsp;DataRange1d(id='1005', ...),</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">x_scale&nbsp;=&nbsp;LinearScale(id='1009', ...),</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">y_range&nbsp;=&nbsp;DataRange1d(id='1007', ...),</div></div><div class=\"1109\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">y_scale&nbsp;=&nbsp;LinearScale(id='1011', ...))</div></div></div>\n",
       "<script>\n",
       "(function() {\n",
       "  let expanded = false;\n",
       "  const ellipsis = document.getElementById(\"1110\");\n",
       "  ellipsis.addEventListener(\"click\", function() {\n",
       "    const rows = document.getElementsByClassName(\"1109\");\n",
       "    for (let i = 0; i < rows.length; i++) {\n",
       "      const el = rows[i];\n",
       "      el.style.display = expanded ? \"none\" : \"table-row\";\n",
       "    }\n",
       "    ellipsis.innerHTML = expanded ? \"&hellip;)\" : \"&lsaquo;&lsaquo;&lsaquo;\";\n",
       "    expanded = !expanded;\n",
       "  });\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "Figure(id='1004', ...)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "draw_vectors(examples[\"transformed_features\"][:, 0],\n",
    "             examples[\"transformed_features\"][:, 1],\n",
    "             color=[[\"red\", \"blue\"][t] for t in examples[\"targets\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fNF6LRQ9MPI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "На визуализации нет четкого разделения твитов между классами. Это значит, что по полученным нами векторам твитов не так-то просто определить, к какому классу твит пренадлежит. Значит, обычный линейный классификатор не очень хорошо справится с задачей. Надо будет делать глубокую (хотя бы два слоя) нейронную сеть.\n",
    "\n",
    "Подготовим загрузчики данных.\n",
    "Усреднение векторов будем делать в \"батчевалке\"(`collate_fn`). Она используется для того, чтобы собирать из данных `torch.Tensor` батчи, которые можно отправлять в модель.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "y1XapsADtITv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "batch_size = 1024\n",
    "# num_workers = 4\n",
    "\n",
    "def average_emb(batch):\n",
    "    features = np.array([np.mean(b[\"feature\"], axis=0) for b in batch])\n",
    "    targets = np.array([b[\"target\"] for b in batch])\n",
    "\n",
    "    return {\"features\": torch.FloatTensor(features), \"targets\": torch.FloatTensor(targets)}\n",
    "\n",
    "\n",
    "train_size = math.ceil(len(dev) * 0.8)\n",
    "train, valid = random_split(dev, [train_size, len(dev) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True, \n",
    "                          drop_last=True, \n",
    "                          collate_fn=average_emb)\n",
    "\n",
    "valid_loader = DataLoader(valid, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False, \n",
    "                          drop_last=False, \n",
    "                          collate_fn=average_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-zs0WEK-Vkt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Определим функции для тренировки и теста модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "U--T2Gjw1r27",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def training(model, optimizer, criterion, train_loader, e, device=device):\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {e + 1}. Train Loss: {0}, Train Acc: {0}\")\n",
    "    mean_loss = 0\n",
    "    mean_acc = 0\n",
    "    model.train()\n",
    "    for batch in pbar:\n",
    "        features = batch[\"features\"].to(device)\n",
    "        targets = batch[\"targets\"].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(features)                                 # Получим предсказания модели\n",
    "        loss = criterion(preds, targets)                        # Посчитаем лосс\n",
    "        acc = accuracy_score(targets.cpu(), preds.cpu() > 0.5)  # Посчитаем точность модели\n",
    "        \n",
    "        mean_loss += loss.item()\n",
    "        mean_acc += acc.item()\n",
    "    \n",
    "        # Обновим параметры модели\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_description(f\"Epoch {e + 1}. Train Loss: {loss:.4}, Train Acc: {acc:.4}\")\n",
    "    \n",
    "    return {\"Train Loss\": mean_loss / len(train_loader), \"Train Acc\": mean_acc / len(train_loader)}\n",
    "    \n",
    "\n",
    "def testing(model, criterion, test_loader, device=device):\n",
    "    pbar = tqdm(test_loader, desc=f\"Test Loss: {0}, Test Acc: {0}\")\n",
    "    mean_loss = 0\n",
    "    mean_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            features = batch[\"features\"].to(device)\n",
    "            targets = batch[\"targets\"].to(device)\n",
    "\n",
    "            preds = model(features)                                 # Получим предсказания модели\n",
    "            loss = criterion(preds, targets)                        # Посчитаем лосс\n",
    "            acc = accuracy_score(targets.cpu(), preds.cpu() > 0.5)  # Посчитаем точность модели\n",
    "\n",
    "            mean_loss += loss.item()\n",
    "            mean_acc += acc.item()\n",
    "\n",
    "            pbar.set_description(f\"Test Loss: {loss:.4}, Test Acc: {acc:.4}\")\n",
    "\n",
    "    return {\"Test Loss\": mean_loss / len(test_loader), \"Test Acc\": mean_acc / len(test_loader)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVg_XBBb-YBH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Создадим модель, оптимизатор и целевую функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "EBoZ4F3Fx1Hm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def init_normal(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.uniform_(m.weight)\n",
    "\n",
    "vector_size = dev.word2vec.vector_size\n",
    "num_classes = 2\n",
    "lr = 1e-2\n",
    "num_epochs = 10\n",
    "\n",
    "model = nn.Sequential(nn.Linear(in_features=vector_size, out_features=vector_size),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(in_features=vector_size, out_features=1),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Flatten(start_dim=0))\n",
    "\n",
    "model.apply(init_normal)\n",
    "model = model.to(device)\n",
    "criterion = nn.BCELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AitU8AR-zBj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Наконец, обучим модель и протестируем её.\n",
    "\n",
    "После каждой эпохи будем проверять качество модели на валидационной части датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "gKhk71Pmx1F1",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1. Train Loss: 0.5283, Train Acc: 0.7441: 100%|██████████| 1000/1000 [02:21<00:00,  7.08it/s]\n",
      "Test Loss: 0.4802, Test Acc: 0.7607: 100%|██████████| 250/250 [00:34<00:00,  7.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 1.1274324446618558, 'Train Acc': 0.7320029296875}\n",
      "{'Test Loss': 0.5172797170877457, 'Test Acc': 0.75400390625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2. Train Loss: 0.5105, Train Acc: 0.7334: 100%|██████████| 1000/1000 [02:23<00:00,  6.97it/s]\n",
      "Test Loss: 0.4736, Test Acc: 0.7578: 100%|██████████| 250/250 [00:35<00:00,  7.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.5088474698662758, 'Train Acc': 0.7552099609375}\n",
      "{'Test Loss': 0.5000405178070069, 'Test Acc': 0.75967578125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3. Train Loss: 0.4493, Train Acc: 0.7812: 100%|██████████| 1000/1000 [02:25<00:00,  6.87it/s]\n",
      "Test Loss: 0.4752, Test Acc: 0.7539: 100%|██████████| 250/250 [00:35<00:00,  7.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.49599207982420923, 'Train Acc': 0.7608427734375}\n",
      "{'Test Loss': 0.49524522650241853, 'Test Acc': 0.7621484375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4. Train Loss: 0.563, Train Acc: 0.7646: 100%|██████████| 1000/1000 [02:20<00:00,  7.13it/s]\n",
      "Test Loss: 0.463, Test Acc: 0.7549: 100%|██████████| 250/250 [00:33<00:00,  7.38it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.49299672231078145, 'Train Acc': 0.762392578125}\n",
      "{'Test Loss': 0.4918857194185257, 'Test Acc': 0.76534765625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5. Train Loss: 0.4776, Train Acc: 0.7646: 100%|██████████| 1000/1000 [02:17<00:00,  7.27it/s]\n",
      "Test Loss: 0.4672, Test Acc: 0.7588: 100%|██████████| 250/250 [00:33<00:00,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.49115170115232465, 'Train Acc': 0.7641611328125}\n",
      "{'Test Loss': 0.4898958712816238, 'Test Acc': 0.76638671875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6. Train Loss: 0.5017, Train Acc: 0.748: 100%|██████████| 1000/1000 [02:19<00:00,  7.17it/s]\n",
      "Test Loss: 0.4759, Test Acc: 0.75: 100%|██████████| 250/250 [00:33<00:00,  7.39it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.489050422757864, 'Train Acc': 0.7655908203125}\n",
      "{'Test Loss': 0.4893279309272766, 'Test Acc': 0.76690625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7. Train Loss: 0.4766, Train Acc: 0.7744: 100%|██████████| 1000/1000 [02:20<00:00,  7.10it/s]\n",
      "Test Loss: 0.4729, Test Acc: 0.751: 100%|██████████| 250/250 [00:33<00:00,  7.36it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.48672347465157506, 'Train Acc': 0.766125}\n",
      "{'Test Loss': 0.4879494584798813, 'Test Acc': 0.767140625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8. Train Loss: 0.4635, Train Acc: 0.7803: 100%|██████████| 1000/1000 [02:19<00:00,  7.17it/s]\n",
      "Test Loss: 0.4716, Test Acc: 0.7568: 100%|██████████| 250/250 [00:33<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.4851794149577618, 'Train Acc': 0.7671484375}\n",
      "{'Test Loss': 0.49004113841056823, 'Test Acc': 0.76766015625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9. Train Loss: 0.4742, Train Acc: 0.7705: 100%|██████████| 1000/1000 [02:19<00:00,  7.18it/s]\n",
      "Test Loss: 0.4652, Test Acc: 0.7607: 100%|██████████| 250/250 [00:33<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.48337178406119347, 'Train Acc': 0.767890625}\n",
      "{'Test Loss': 0.4853319125175476, 'Test Acc': 0.7686953125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10. Train Loss: 0.4647, Train Acc: 0.7764: 100%|██████████| 1000/1000 [02:19<00:00,  7.18it/s]\n",
      "Test Loss: 0.4679, Test Acc: 0.7568: 100%|██████████| 250/250 [00:34<00:00,  7.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.4823636828958988, 'Train Acc': 0.7680234375}\n",
      "{'Test Loss': 0.4899590277671814, 'Test Acc': 0.76875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_metric = np.inf\n",
    "for e in range(num_epochs):\n",
    "    train_log = training(model, optimizer, criterion, train_loader, e, device)\n",
    "    log = testing(model, criterion, valid_loader, device)\n",
    "    print(train_log)\n",
    "    print(log)\n",
    "    if log[\"Test Loss\"] < best_metric:\n",
    "        torch.save(model.state_dict(), \"model.pt\")\n",
    "        best_metric = log[\"Test Loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "di4dGwD4x1Dt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5879, Test Acc: 0.7324: 100%|██████████| 313/313 [00:42<00:00,  7.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.490078897997975, 'Test Acc': 0.7657966004392971}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(\n",
    "    TwitterDataset(test_data, \"text\", \"emotion\", word2vec),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=average_emb)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model.pt\", map_location=device))\n",
    "\n",
    "print(testing(model, criterion, test_loader, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRvzpldHSAu0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Embeddings for unknown words (8 баллов)\n",
    "\n",
    "Пока что использовалась не вся информация из текста. Часть информации фильтровалось – если слова не было в словаре эмбеддингов, то мы просто превращали слово в нулевой вектор. Хочется использовать информацию по-максимуму. Поэтому рассмотрим другие способы обработки слов, которых нет в словаре. А именно:\n",
    "\n",
    "- Для каждого незнакомого слова будем запоминать его контекст(слова слева и справа от этого слова). Эмбеддингом нашего незнакомого слова будет сумма эмбеддингов всех слов из его контекста.\n",
    "- Для каждого слова текста получим его эмбеддинг из Tfidf с помощью ```TfidfVectorizer``` из [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer). Итоговым эмбеддингом для каждого слова будет сумма двух эмбеддингов: предобученного и Tfidf-ного. Для слов, которых нет в словаре предобученных эмбеддингов, результирующий эмбеддинг будет просто полученный из Tfidf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RxhEpKalU1UQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Вариант 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Сформируем класс датасета с учётом контекста. В качестве контекста возмём `3` слова с каждой стороны от незнакомого слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TwitterDataset_context(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec):\n",
    "        self.tokenizer = nltk.WordPunctTokenizer()\n",
    "        self.data = data\n",
    "        self.feature_column = feature_column\n",
    "        self.target_column = target_column\n",
    "        self.word2vec = word2vec\n",
    "        self.label2num = lambda label: 0 if label == 0 else 1\n",
    "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
    "        self.std = np.std(word2vec.vectors, axis=0)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.data[self.feature_column][item]\n",
    "        label = self.label2num(self.data[self.target_column][item])\n",
    "        tokens = self.get_tokens_(text)\n",
    "        embeddings = self.get_embeddings_(tokens)\n",
    "\n",
    "        return {\"feature\": embeddings, \"target\": label}\n",
    "\n",
    "    def get_tokens_(self, text):\n",
    "        text = re.sub(r'@\\S+', '', text)           # уберём ники вида @chrishasboobs\n",
    "        text = re.sub(r'http\\S+', '', text)        # уберём http ссылки\n",
    "        text = re.sub('[^A-Za-z0-9]+', ' ', text)  # уберём слова в неверной кодировке\n",
    "        # заменим все символы, которые повторяются >3 раз подряд на единичные (stoppp -> stop) \n",
    "        text = re.sub(r'([A-Za-z0-9])\\1(?=\\1)', '', text)  \n",
    "        line = self.tokenizer.tokenize(text.lower())\n",
    "        filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) > 2]\n",
    "        \n",
    "        return filtered_line\n",
    "    \n",
    "    def get_vector_(self, token):\n",
    "        return (self.word2vec.get_vector(token) - self.mean) / self.std\n",
    "\n",
    "    def get_embeddings_(self, tokens):\n",
    "        window_size = 3\n",
    "        embeddings = []                                          \n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in self.word2vec:\n",
    "                embeddings.append(self.get_vector_(token))\n",
    "            else:\n",
    "                context = tokens[max(i-window_size, 0):min(i+window_size, len(tokens))]\n",
    "                context.remove(token)\n",
    "                context_embedding = np.sum(np.array([self.get_vector_(w) for w in context if w in self.word2vec]), axis=0)\n",
    "                if context_embedding.all() == 0:\n",
    "                    continue\n",
    "                embeddings.append(context_embedding)\n",
    "                \n",
    "        if len(embeddings) == 0:\n",
    "            embeddings = np.zeros((1, self.word2vec.vector_size))\n",
    "        else:\n",
    "            embeddings = np.array(embeddings)\n",
    "            if len(embeddings.shape) == 1:\n",
    "                embeddings = embeddings.reshape(-1, 1)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Сформируем датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dev_context = TwitterDataset_context(dev_data, \"text\", \"emotion\", word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Разобъём датасет на тренировочную и валидационную части и создадим даталоадеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_size = math.ceil(len(dev_context) * 0.8)\n",
    "\n",
    "train, valid = random_split(dev_context, [train_size, len(dev_context) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True, \n",
    "                          drop_last=True, \n",
    "                          collate_fn=average_emb)\n",
    "\n",
    "valid_loader = DataLoader(valid, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False, \n",
    "                          drop_last=False, \n",
    "                          collate_fn=average_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Создадим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(in_features=vector_size, out_features=vector_size),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(in_features=vector_size, out_features=1),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Flatten(start_dim=0))\n",
    "\n",
    "model.apply(init_normal)\n",
    "model = model.to(device)\n",
    "criterion = nn.BCELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Проведём обучение модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1. Train Loss: 0.4931, Train Acc: 0.7676: 100%|██████████| 1000/1000 [02:49<00:00,  5.90it/s]\n",
      "Test Loss: 0.5056, Test Acc: 0.7363: 100%|██████████| 250/250 [00:41<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 1.1624080924093723, 'Train Acc': 0.730255859375}\n",
      "{'Test Loss': 0.5437137211561203, 'Test Acc': 0.74480859375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2. Train Loss: 0.5226, Train Acc: 0.749: 100%|██████████| 1000/1000 [02:50<00:00,  5.85it/s]\n",
      "Test Loss: 0.5061, Test Acc: 0.752: 100%|██████████| 250/250 [00:41<00:00,  5.95it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.5221134818196297, 'Train Acc': 0.7504248046875}\n",
      "{'Test Loss': 0.5169343272447586, 'Test Acc': 0.7504375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3. Train Loss: 0.4846, Train Acc: 0.7734: 100%|██████████| 1000/1000 [02:48<00:00,  5.93it/s]\n",
      "Test Loss: 0.5092, Test Acc: 0.7529: 100%|██████████| 250/250 [00:41<00:00,  6.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.510260365754366, 'Train Acc': 0.7549599609375}\n",
      "{'Test Loss': 0.5125782581567764, 'Test Acc': 0.75380078125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4. Train Loss: 0.5116, Train Acc: 0.751: 100%|██████████| 1000/1000 [02:49<00:00,  5.89it/s]\n",
      "Test Loss: 0.4973, Test Acc: 0.7617: 100%|██████████| 250/250 [00:41<00:00,  6.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.5053712334036827, 'Train Acc': 0.7577216796875}\n",
      "{'Test Loss': 0.5081424814462662, 'Test Acc': 0.757578125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5. Train Loss: 0.501, Train Acc: 0.752: 100%|██████████| 1000/1000 [02:49<00:00,  5.91it/s] \n",
      "Test Loss: 0.4969, Test Acc: 0.7686: 100%|██████████| 250/250 [00:41<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.4990002714693546, 'Train Acc': 0.7599423828125}\n",
      "{'Test Loss': 0.5065571092367173, 'Test Acc': 0.75688671875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6. Train Loss: 0.505, Train Acc: 0.7422: 100%|██████████| 1000/1000 [02:48<00:00,  5.92it/s]\n",
      "Test Loss: 0.4922, Test Acc: 0.7676: 100%|██████████| 250/250 [00:41<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.49755491518974304, 'Train Acc': 0.7607607421875}\n",
      "{'Test Loss': 0.5040187801122665, 'Test Acc': 0.7581171875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7. Train Loss: 0.4796, Train Acc: 0.7852: 100%|██████████| 1000/1000 [02:48<00:00,  5.94it/s]\n",
      "Test Loss: 0.4998, Test Acc: 0.7607: 100%|██████████| 250/250 [00:41<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.4968789222538471, 'Train Acc': 0.76175}\n",
      "{'Test Loss': 0.5033802341222763, 'Test Acc': 0.75877734375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8. Train Loss: 0.5013, Train Acc: 0.7412: 100%|██████████| 1000/1000 [02:48<00:00,  5.93it/s]\n",
      "Test Loss: 0.5028, Test Acc: 0.7607: 100%|██████████| 250/250 [00:41<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.4954324324131012, 'Train Acc': 0.7619189453125}\n",
      "{'Test Loss': 0.5138514807224274, 'Test Acc': 0.75469140625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9. Train Loss: 0.509, Train Acc: 0.7344: 100%|██████████| 1000/1000 [02:49<00:00,  5.91it/s]\n",
      "Test Loss: 0.492, Test Acc: 0.7637: 100%|██████████| 250/250 [00:41<00:00,  6.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.49633976665139196, 'Train Acc': 0.7625283203125}\n",
      "{'Test Loss': 0.5029434760808945, 'Test Acc': 0.7585625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10. Train Loss: 0.5121, Train Acc: 0.752: 100%|██████████| 1000/1000 [02:47<00:00,  5.98it/s]\n",
      "Test Loss: 0.4968, Test Acc: 0.7656: 100%|██████████| 250/250 [00:41<00:00,  6.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.4939254277944565, 'Train Acc': 0.7628408203125}\n",
      "{'Test Loss': 0.5072914344072342, 'Test Acc': 0.75830859375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_metric = np.inf\n",
    "for e in range(num_epochs):\n",
    "    train_log = training(model, optimizer, criterion, train_loader, e, device)\n",
    "    log = testing(model, criterion, valid_loader, device)\n",
    "    print(train_log)\n",
    "    print(log)\n",
    "    if log[\"Test Loss\"] < best_metric:\n",
    "        torch.save(model.state_dict(), \"model_context.pt\")\n",
    "        best_metric = log[\"Test Loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Проверим качество модели на тестовых данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6046, Test Acc: 0.7188: 100%|██████████| 313/313 [00:51<00:00,  6.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5028491101135462, 'Test Acc': 0.7595129043530351}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(\n",
    "    TwitterDataset_context(test_data, \"text\", \"emotion\", word2vec),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=average_emb)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model_context.pt\", map_location=device))\n",
    "\n",
    "print(testing(model, criterion, test_loader, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Вариант 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Токенизируем полный датасет в целях подготовки данных для `TfidfVectorizer`, избавимся от знаков пунктуации и выкинем все слова, состоящие менее чем из 4 букв:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1600000/1600000 [00:59<00:00, 26816.64it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_data = []\n",
    "tokenizer = nltk.WordPunctTokenizer()\n",
    "\n",
    "for i in tqdm(range(len(data))):\n",
    "    text = data[\"text\"][i]\n",
    "    text = re.sub(r'@\\S+', '', text)           # уберём ники вида @chrishasboobs\n",
    "    text = re.sub(r'http\\S+', '', text)        # уберём http ссылки\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text)  # уберём слова в неверной кодировке\n",
    "    # заменим все символы, которые повторяются >3 раз подряд на единичные (stoppp -> stop)\n",
    "    text = re.sub(r'([A-Za-z0-9])\\1(?=\\1)', '',text)\n",
    "    line = tokenizer.tokenize(text.lower())    # приведём к нижнему регистру\n",
    "    filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) > 2]\n",
    "    filtered_string = \" \".join(filtered_line)\n",
    "    tokenized_data.append(filtered_string)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1600000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Выведем несколько строк подготовленных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "that bummer you shoulda got david carr third day\n",
      "upset that can update his facebook texting and might cry result school today also blah\n",
      "dived many times for the ball managed save the rest out bounds\n",
      "whole body feels itchy and like its fire\n",
      "not behaving all mad why here because can see you all over there\n",
      "not the whole crew\n",
      "need hug\n",
      "hey long time see yes rains bit only bit lol fine thanks how you\n",
      "nope they didn have\n",
      "que muera\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(tokenized_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Обучим `TfidfVectorizer` на полном датасете:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 257832)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase=False)\n",
    "vectors = vectorizer.fit_transform(tokenized_data)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Так как оперируем векторами размера `300`, выполним `SVD` преобразование для уменьшения размерности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 300)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    " \n",
    "trun_svd = TruncatedSVD(n_components=300)\n",
    "vectors_transformed = trun_svd.fit_transform(vectors)\n",
    "vectors_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Подготовим класс датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TwitterDataset_tfidf(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, \n",
    "                 word2vec: gensim.models.Word2Vec, tfidf, t_vectors: np.array):\n",
    "        self.tokenizer = nltk.WordPunctTokenizer()\n",
    "        \n",
    "        self.data = data\n",
    "\n",
    "        self.feature_column = feature_column\n",
    "        self.target_column = target_column\n",
    "\n",
    "        self.word2vec = word2vec\n",
    "        self.tfidf = tfidf\n",
    "        self.t_vectors = t_vectors\n",
    "\n",
    "        self.label2num = lambda label: 0 if label == 0 else 1\n",
    "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
    "        self.std = np.std(word2vec.vectors, axis=0)\n",
    "        \n",
    "        self.mean_tfidf = np.mean(t_vectors, axis=0)\n",
    "        self.std_tfidf = np.std(t_vectors, axis=0)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.data[self.feature_column][item]\n",
    "        label = self.label2num(self.data[self.target_column][item])\n",
    "\n",
    "        tokens = self.get_tokens_(text)\n",
    "        embeddings = self.get_embeddings_(tokens)\n",
    "\n",
    "        return {\"feature\": embeddings, \"target\": label}\n",
    "\n",
    "    def get_tokens_(self, text):\n",
    "        text = re.sub(r'@\\S+', '', text)           # уберём ники вида @chrishasboobs\n",
    "        text = re.sub(r'http\\S+', '', text)        # уберём http ссылки\n",
    "        text = re.sub('[^A-Za-z0-9]+', ' ', text)  # уберём слова в неверной кодировке\n",
    "        # заменим все символы, которые повторяются >3 раз подряд на единичные (stoppp -> stop)\n",
    "        text = re.sub(r'([A-Za-z0-9])\\1(?=\\1)', '', text)\n",
    "        line = self.tokenizer.tokenize(text.lower())\n",
    "        filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) > 2]\n",
    "        \n",
    "        return filtered_line\n",
    "    \n",
    "    def get_w2v_vector_(self, token):\n",
    "        return (self.word2vec.get_vector(token) - self.mean) / self.std\n",
    "    \n",
    "    def get_tfidf_vector_(self, token):\n",
    "        tfidf_idx = self.tfidf.vocabulary_[token]\n",
    "        tfidf_vector = self.t_vectors[tfidf_idx]\n",
    "        return (tfidf_vector - self.mean_tfidf) / self.std_tfidf\n",
    "\n",
    "    def get_embeddings_(self, tokens):\n",
    "        embeddings = []\n",
    "        \n",
    "        for token in tokens:\n",
    "            if token in self.word2vec:\n",
    "                w2v_emb = self.get_w2v_vector_(token)\n",
    "                tfidf_emb = self.get_tfidf_vector_(token)\n",
    "                embeddings.append(w2v_emb + tfidf_emb)\n",
    "            else:\n",
    "                tfidf_emb = self.get_tfidf_vector_(token)\n",
    "                embeddings.append(tfidf_emb)\n",
    "\n",
    "        if len(embeddings) == 0:\n",
    "            embeddings = np.zeros((1, self.word2vec.vector_size))\n",
    "        else:\n",
    "            embeddings = np.array(embeddings)\n",
    "            if len(embeddings.shape) == 1:\n",
    "                embeddings = embeddings.reshape(-1, 1)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Сформируем датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dev_tfidf = TwitterDataset_tfidf(dev_data, \"text\", \"emotion\", word2vec, vectorizer, vectors_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Разобъём датасет на тренировочную и валидационную части и создадим даталоадеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_size = math.ceil(len(dev_tfidf) * 0.8)\n",
    "\n",
    "train, valid = random_split(dev_tfidf, [train_size, len(dev_tfidf) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True, \n",
    "                          drop_last=True, \n",
    "                          collate_fn=average_emb)\n",
    "\n",
    "valid_loader = DataLoader(valid, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False, \n",
    "                          drop_last=False, \n",
    "                          collate_fn=average_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Создадим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(in_features=vector_size, out_features=vector_size),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(in_features=vector_size, out_features=1),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Flatten(start_dim=0))\n",
    "\n",
    "model.apply(init_normal)\n",
    "model = model.to(device)\n",
    "criterion = nn.BCELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Проведём обучение модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1. Train Loss: 0.7224, Train Acc: 0.7344: 100%|██████████| 1000/1000 [03:22<00:00,  4.94it/s]\n",
      "Test Loss: 0.5387, Test Acc: 0.7354: 100%|██████████| 250/250 [00:49<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 1.2678647902607918, 'Train Acc': 0.718419921875}\n",
      "{'Test Loss': 0.6072666885852813, 'Test Acc': 0.73495703125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2. Train Loss: 0.5381, Train Acc: 0.7305: 100%|██████████| 1000/1000 [03:21<00:00,  4.96it/s]\n",
      "Test Loss: 0.5129, Test Acc: 0.7529: 100%|██████████| 250/250 [00:49<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.5442264665961265, 'Train Acc': 0.7409833984375}\n",
      "{'Test Loss': 0.5295983834266662, 'Test Acc': 0.74490625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3. Train Loss: 0.6722, Train Acc: 0.7773: 100%|██████████| 1000/1000 [03:20<00:00,  4.99it/s]\n",
      "Test Loss: 0.5097, Test Acc: 0.7578: 100%|██████████| 250/250 [00:49<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.5305461786687374, 'Train Acc': 0.7479951171875}\n",
      "{'Test Loss': 0.529522411942482, 'Test Acc': 0.749296875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4. Train Loss: 0.5809, Train Acc: 0.7568: 100%|██████████| 1000/1000 [03:22<00:00,  4.93it/s]\n",
      "Test Loss: 0.5048, Test Acc: 0.7539: 100%|██████████| 250/250 [00:48<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.5264951276481151, 'Train Acc': 0.75102734375}\n",
      "{'Test Loss': 0.5264121829271317, 'Test Acc': 0.74966015625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5. Train Loss: 0.6044, Train Acc: 0.7549: 100%|██████████| 1000/1000 [03:19<00:00,  5.01it/s]\n",
      "Test Loss: 0.4945, Test Acc: 0.7656: 100%|██████████| 250/250 [00:49<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.5204642165899277, 'Train Acc': 0.7532919921875}\n",
      "{'Test Loss': 0.5299831327199936, 'Test Acc': 0.7534140625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6. Train Loss: 0.4914, Train Acc: 0.748: 100%|██████████| 1000/1000 [03:19<00:00,  5.01it/s]\n",
      "Test Loss: 0.5009, Test Acc: 0.7598: 100%|██████████| 250/250 [00:49<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.5191093256473541, 'Train Acc': 0.7551748046875}\n",
      "{'Test Loss': 0.5316073104143143, 'Test Acc': 0.7537265625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7. Train Loss: 0.592, Train Acc: 0.748: 100%|██████████| 1000/1000 [03:19<00:00,  5.01it/s] \n",
      "Test Loss: 0.4911, Test Acc: 0.7646: 100%|██████████| 250/250 [00:49<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.5163052897751331, 'Train Acc': 0.75627734375}\n",
      "{'Test Loss': 0.5242790454626083, 'Test Acc': 0.754046875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8. Train Loss: 0.4593, Train Acc: 0.7764: 100%|██████████| 1000/1000 [03:21<00:00,  4.97it/s]\n",
      "Test Loss: 0.5043, Test Acc: 0.7607: 100%|██████████| 250/250 [00:49<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.5140500944554806, 'Train Acc': 0.7573525390625}\n",
      "{'Test Loss': 0.54930238199234, 'Test Acc': 0.7553125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9. Train Loss: 0.5708, Train Acc: 0.7764: 100%|██████████| 1000/1000 [03:19<00:00,  5.01it/s]\n",
      "Test Loss: 0.4943, Test Acc: 0.7646: 100%|██████████| 250/250 [00:49<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.5157421705722809, 'Train Acc': 0.7583408203125}\n",
      "{'Test Loss': 0.5327710771560669, 'Test Acc': 0.755296875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10. Train Loss: 0.517, Train Acc: 0.7354: 100%|██████████| 1000/1000 [03:21<00:00,  4.97it/s]\n",
      "Test Loss: 0.5034, Test Acc: 0.7627: 100%|██████████| 250/250 [00:49<00:00,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.5126367146968842, 'Train Acc': 0.7591826171875}\n",
      "{'Test Loss': 0.5357319663763046, 'Test Acc': 0.756015625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_metric = np.inf\n",
    "for e in range(num_epochs):\n",
    "    train_log = training(model, optimizer, criterion, train_loader, e, device)\n",
    "    log = testing(model, criterion, valid_loader, device)\n",
    "    print(train_log)\n",
    "    print(log)\n",
    "    if log[\"Test Loss\"] < best_metric:\n",
    "        torch.save(model.state_dict(), \"model_tfidf.pt\")\n",
    "        best_metric = log[\"Test Loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Проверим качество модели на тестовых данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5745, Test Acc: 0.7363: 100%|██████████| 313/313 [01:02<00:00,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5289415219150031, 'Test Acc': 0.7536722494009584}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(\n",
    "    TwitterDataset_tfidf(test_data, \"text\", \"emotion\", word2vec, vectorizer, vectors_transformed),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=average_emb)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model_tfidf.pt\", map_location=device))\n",
    "\n",
    "print(testing(model, criterion, test_loader, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Выводы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Для адекватного сравнения была выполнена идентичная токенизация для всех трёх моделей. А именно, выполнена следующая фильтрация:\n",
    "- удалены ники пользователей\n",
    "- удаленны http ссылки на web ресурсы\n",
    "- удалены все слова и символы в кодировках отличных от исходной\n",
    "- символы, которые повторяются более 3 раз заменены на единичные (stoppp -> stop)\n",
    "- текст приведён к нижнему регистру\n",
    "- удалены знаки пунктуации\n",
    "- удалены все слова состоящие менее из 3-х символов  \n",
    "\n",
    "В эти условиях наилучшее качество показала базовая модель ('Test Acc': 0.765). Следом за ней по точности идёт модель, в которой эмбеддингом незнакомого слова будет сумма эмбеддингов всех слов из его контекста ('Test Acc': 0.758). Хуже всех показала себя модель с применением эмбеддингов из Tfidf ('Test Acc': 0.753). В данном случае дополнительные техники обработки слов, которых нет в словаре, не принесли пользы. Возможно, суммирование контекста и добавление эмбеддингов Tfidf только вносят искажения, тем самым негативно влияя на результат. Потому что норма суммы векторов контекста для незнакомого слова будет значительно выше нормы вектора известного слова (вариант 1), а норма известных слов (вариант 2) полученной в виде суммы обычного эмбеддинга и tfidf будет больше нормы tfidf вектора неизвестных слова. Не говоря уже о том, что tfidf векторизованы по другому принципу.\n",
    "Тем не менее, значения точностей всех трёх моделей близки. \n",
    "\n",
    "\n",
    "Стоит отметить, что при изменении принципов токенизации и фильтрации токенов, можно добиться иного распределения результатов между моделями, особенно если настраивать эти параметры для каждой модели в отдельности. Но я ставил цель сравнения моделей в одинаковых условиях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Исправим недостатки предыдущих подходов. Для каждого незнакомого слова будем запоминать его контекст(слова слева и справа от этого слова). Эмбеддингом нашего незнакомого слова будет СРЕДНЕЕ эмбеддингов всех слов из его контекста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TwitterDataset_context_mean(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, feature_column: str, target_column: str, word2vec: gensim.models.Word2Vec):\n",
    "        self.tokenizer = nltk.WordPunctTokenizer()\n",
    "        self.data = data\n",
    "        self.feature_column = feature_column\n",
    "        self.target_column = target_column\n",
    "        self.word2vec = word2vec\n",
    "        self.label2num = lambda label: 0 if label == 0 else 1\n",
    "        self.mean = np.mean(word2vec.vectors, axis=0)\n",
    "        self.std = np.std(word2vec.vectors, axis=0)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = self.data[self.feature_column][item]\n",
    "        label = self.label2num(self.data[self.target_column][item])\n",
    "        tokens = self.get_tokens_(text)\n",
    "        embeddings = self.get_embeddings_(tokens)\n",
    "\n",
    "        return {\"feature\": embeddings, \"target\": label}\n",
    "\n",
    "    def get_tokens_(self, text):\n",
    "        text = re.sub(r'@\\S+', '', text)           # уберём ники вида @chrishasboobs\n",
    "        text = re.sub(r'http\\S+', '', text)        # уберём http ссылки\n",
    "        text = re.sub('[^A-Za-z0-9]+', ' ', text)  # уберём слова в неверной кодировке\n",
    "        # заменим все символы, которые повторяются >3 раз подряд на единичные (stoppp -> stop) \n",
    "        text = re.sub(r'([A-Za-z0-9])\\1(?=\\1)', '', text)  \n",
    "        line = self.tokenizer.tokenize(text.lower())\n",
    "        filtered_line = [w for w in line if all(c not in string.punctuation for c in w) and len(w) > 2]\n",
    "        \n",
    "        return filtered_line\n",
    "    \n",
    "    def get_vector_(self, token):\n",
    "        return (self.word2vec.get_vector(token) - self.mean) / self.std\n",
    "\n",
    "    def get_embeddings_(self, tokens):\n",
    "        window_size = 3\n",
    "        embeddings = []                                          \n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            if token in self.word2vec:\n",
    "                embeddings.append(self.get_vector_(token))\n",
    "            else:\n",
    "                context = tokens[max(i-window_size, 0):min(i+window_size, len(tokens))]\n",
    "                context.remove(token)\n",
    "                context_embeddings = np.array([self.get_vector_(w) for w in context if w in self.word2vec])\n",
    "                context_embedding = np.sum(context_embeddings, axis=0)\n",
    "                if context_embedding.all() == 0:\n",
    "                    continue\n",
    "                embeddings.append(context_embedding / len(context_embeddings))\n",
    "                \n",
    "        if len(embeddings) == 0:\n",
    "            embeddings = np.zeros((1, self.word2vec.vector_size))\n",
    "        else:\n",
    "            embeddings = np.array(embeddings)\n",
    "            if len(embeddings.shape) == 1:\n",
    "                embeddings = embeddings.reshape(-1, 1)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Сформируем датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dev_context_mean = TwitterDataset_context_mean(dev_data, \"text\", \"emotion\", word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Разобъём датасет на тренировочную и валидационную части и создадим даталоадеры:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_size = math.ceil(len(dev_context_mean) * 0.8)\n",
    "\n",
    "train, valid = random_split(dev_context_mean, [train_size, len(dev_context_mean) - train_size])\n",
    "\n",
    "train_loader = DataLoader(train, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True, \n",
    "                          drop_last=True, \n",
    "                          collate_fn=average_emb)\n",
    "\n",
    "valid_loader = DataLoader(valid, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=False, \n",
    "                          drop_last=False, \n",
    "                          collate_fn=average_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Создадим модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(in_features=vector_size, out_features=vector_size),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(in_features=vector_size, out_features=1),\n",
    "                      nn.Sigmoid(),\n",
    "                      nn.Flatten(start_dim=0))\n",
    "\n",
    "model.apply(init_normal)\n",
    "model = model.to(device)\n",
    "criterion = nn.BCELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Проведём обучение модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1. Train Loss: 0.5466, Train Acc: 0.7461: 100%|██████████| 1000/1000 [02:45<00:00,  6.03it/s]\n",
      "Test Loss: 0.5943, Test Acc: 0.7627: 100%|██████████| 250/250 [00:40<00:00,  6.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 1.0940055283606052, 'Train Acc': 0.730822265625}\n",
      "{'Test Loss': 0.5169219068288803, 'Test Acc': 0.7517578125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2. Train Loss: 0.5058, Train Acc: 0.7412: 100%|██████████| 1000/1000 [02:45<00:00,  6.03it/s]\n",
      "Test Loss: 0.5862, Test Acc: 0.7598: 100%|██████████| 250/250 [00:40<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.5090792016983032, 'Train Acc': 0.7549365234375}\n",
      "{'Test Loss': 0.5094855724573135, 'Test Acc': 0.75663671875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3. Train Loss: 0.5033, Train Acc: 0.7588: 100%|██████████| 1000/1000 [02:45<00:00,  6.05it/s]\n",
      "Test Loss: 0.4861, Test Acc: 0.7676: 100%|██████████| 250/250 [00:41<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.49944290360808374, 'Train Acc': 0.7596416015625}\n",
      "{'Test Loss': 0.5044105075597763, 'Test Acc': 0.757875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4. Train Loss: 0.4843, Train Acc: 0.7598: 100%|██████████| 1000/1000 [02:45<00:00,  6.04it/s]\n",
      "Test Loss: 0.4947, Test Acc: 0.7646: 100%|██████████| 250/250 [00:41<00:00,  6.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.4929694667160511, 'Train Acc': 0.761822265625}\n",
      "{'Test Loss': 0.49779571533203126, 'Test Acc': 0.7602890625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5. Train Loss: 0.488, Train Acc: 0.7676: 100%|██████████| 1000/1000 [02:45<00:00,  6.04it/s]\n",
      "Test Loss: 0.5726, Test Acc: 0.7783: 100%|██████████| 250/250 [00:40<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.49034023693203926, 'Train Acc': 0.7636943359375}\n",
      "{'Test Loss': 0.4996359726190567, 'Test Acc': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6. Train Loss: 0.4989, Train Acc: 0.752: 100%|██████████| 1000/1000 [02:46<00:00,  6.00it/s]\n",
      "Test Loss: 0.4884, Test Acc: 0.7734: 100%|██████████| 250/250 [00:41<00:00,  5.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.4893637880384922, 'Train Acc': 0.764287109375}\n",
      "{'Test Loss': 0.49464541947841645, 'Test Acc': 0.76305078125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7. Train Loss: 0.4716, Train Acc: 0.7744: 100%|██████████| 1000/1000 [02:51<00:00,  5.83it/s]\n",
      "Test Loss: 0.4868, Test Acc: 0.7656: 100%|██████████| 250/250 [00:41<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.487091168910265, 'Train Acc': 0.765060546875}\n",
      "{'Test Loss': 0.49151066434383395, 'Test Acc': 0.76396484375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8. Train Loss: 0.4902, Train Acc: 0.7783: 100%|██████████| 1000/1000 [02:49<00:00,  5.91it/s]\n",
      "Test Loss: 0.5797, Test Acc: 0.7734: 100%|██████████| 250/250 [00:41<00:00,  6.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.48751047483086585, 'Train Acc': 0.7658994140625}\n",
      "{'Test Loss': 0.5020015186071396, 'Test Acc': 0.7627421875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9. Train Loss: 0.4836, Train Acc: 0.7598: 100%|██████████| 1000/1000 [02:45<00:00,  6.03it/s]\n",
      "Test Loss: 0.4739, Test Acc: 0.7715: 100%|██████████| 250/250 [00:40<00:00,  6.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.4867734566926956, 'Train Acc': 0.7664931640625}\n",
      "{'Test Loss': 0.4941212521791458, 'Test Acc': 0.76447265625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10. Train Loss: 0.5117, Train Acc: 0.7559: 100%|██████████| 1000/1000 [02:56<00:00,  5.66it/s]\n",
      "Test Loss: 0.4767, Test Acc: 0.7666: 100%|██████████| 250/250 [00:42<00:00,  5.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train Loss': 0.48500264251232145, 'Train Acc': 0.7672919921875}\n",
      "{'Test Loss': 0.49438302135467527, 'Test Acc': 0.76441796875}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_metric = np.inf\n",
    "for e in range(num_epochs):\n",
    "    train_log = training(model, optimizer, criterion, train_loader, e, device)\n",
    "    log = testing(model, criterion, valid_loader, device)\n",
    "    print(train_log)\n",
    "    print(log)\n",
    "    if log[\"Test Loss\"] < best_metric:\n",
    "        torch.save(model.state_dict(), \"model_context_mean.pt\")\n",
    "        best_metric = log[\"Test Loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Проверим качество модели на тестовых данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5505, Test Acc: 0.7285: 100%|██████████| 313/313 [00:50<00:00,  6.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Test Loss': 0.5032407702348484, 'Test Acc': 0.7608888278753994}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_loader = DataLoader(\n",
    "    TwitterDataset_context(test_data, \"text\", \"emotion\", word2vec), \n",
    "    batch_size=batch_size,  \n",
    "    shuffle=False,\n",
    "    drop_last=False, \n",
    "    collate_fn=average_emb)\n",
    "\n",
    "model.load_state_dict(torch.load(\"model_context_mean.pt\", map_location=device))\n",
    "\n",
    "print(testing(model, criterion, test_loader, device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Полученное качество модели лучше чем подходы из 1 и 2 вариантов, но всё равно не дотягивает по качеству до исходного варинта без дополнительных техник обработки слов, которых нет в словаре."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[homework]embeddings.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
